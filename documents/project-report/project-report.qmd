---
title: "Using computer vision and AI techniques to improve crowd safety management"
subtitle: "Final report"
authors:
  - name: Anton Irvold
    custom-role: "Author"
    email: anirv20@student.sdu.dk
    affiliations:
      - id: SDU
        name: University of Southern Denmark
        department: Technical Faculty
  - name: Christoffer Krath
    email: chkra19@student.sdu.dk
    custom-role: "Author"
    affiliations: 
      - id: sdu
        name: University of Southern Denmark
        department: Technical Faculty
  - name: Sune Lundø Sørensen
    email: slso@mmmi.sdu.dk
    custom-role: "Supervisor"
    affiliations: 
      - id: sdu1
        name: University of Southern Denmark
        department: MMMI
  - name: Mikkel Baun Kjærgaard
    email: mbkj@mmmi.sdu.dk
    custom-role: "Supervisor"
    affiliations: 
      - id: sdu1
        name: University of Southern Denmark
        department: MMMI
  - name: Sofie Dahl
    email: sofie@eventsafety.dk
    custom-role: "Collaborator"
    affiliations: 
      - id: es
        name: Event Safety
  - name: Christian Sejlund
    email: christian@eventsafety.dk
    custom-role: "Collaborator"
    affiliations: 
      - id: es
        name: Event Safety
date: "2024 Jan. 02"
abstract: |
  Template abstract lorem ispum
bibliography: citations.bib
format:
  hikmah-pdf:
    toc: true
    number-sections: true

---
{{< pagebreak >}}

# Acronyms and definitions

| **Acronyms**  | **Definition** |
|--- |--- |
| CSMS              | Crowd Safety Management System |
||
| Crowd Counting              | The defintion of crowd counting in this report follows the definition from @chan2008: "[...] a privacy-preserving system for estimating the size of inhomogeneous crowds, [...] without using explicit object segmentation or tracking" |
||
| ANN               | Artificial Neural Network |
||
| CNN               | Convolutional Neural Network |
||
| Crowd Surges     | A dangerous crowd situation where your movement is controlled by the crowd and not your own actions. |
||
| Mosh pit         | A crowd situation where people vacate an area in the crowd followed by violent or aggressive dancing. Sizes of Mosh pits vary greatly. |
||
| Technical user   | A developer or technician that can implement the CSMS in the technical setup of a concert or venue |
||
| Non-technical user | A security guard or someone viewing crowd footage through the CSMS without any technical or software-related background |
||
| Choke point      | Point with high crowd density and crowd influx           |
||
| SASNet      | Scale-adaptive selection network @sasnet           |
||

{{< pagebreak >}}

# Introduction

## Context

![Fatalities in crowds graph [Figure from @FELICIANI2023106174]](../images/fatalities.jpg){#fig-fatalities}

As seen from @fig-fatalities accidents, fatalities and injuries at festivals and concerts worldwide are on the rise, with just below 5,000 worldwide deaths in the decade of 2010-2019. This can be attributed to several causes, according to @inproceedings: First of all, an increase in the popularity of outdoor music festivals resulting in more and larger festivals with large crowds. Also, high-risk behavior among crowd attendants and the performing artist’s music, behavior, and stage show affects the crowd's safety. Cultural influence has also played a large part in the safety of outdoor musical events. This can be behavior such as crowd surfing, moshing, stage diving and the like. Sudden panic in a crowded place can thus affect the safety of the crowd. 

It is worth mentioning that these figures and descriptions of situations are from media outlets. As such, it's not the full story. While these statistics might make it seem that general safety and incidents at festivals and the like have gotten worse, it is the opposite, It is generally incredibly safe to attend these events, but using newer technologies we will attempt to enhance it further.

At the same time, festivals in Denmark and the rest of the world have had a focus on mitigating the safety risk in large crowds. This can be seen by the establishment of the [Event Safety Foundation](https://eventsafety.dk/historie) in 2015. This is a joint venture between the Skanderborg Festival Group (Smukfest) and Muskelsvindfonden (Grøn Koncert). This foundation has the purpose of securing the safety of the crowds at these two large Danish festivals, as well as many other events in Denmark. They do this through knowledge sharing, professionals and trained volunteers, courses and counseling. This is the company that this project will be done in collaboration with.

To have the best possible outcome for this project, Event Safety invited us to Grøn Koncert and Smukfest in July and August to see how they work and gather security video footage of real crowds at festivals to use as training data. We will also use Event Safety for their practical and theoretical knowledge of crowd safety to use in the system, user feedback and user testing. 

## Problem Statement

Can computer vision software and AI techniques be leveraged to improve crowd overview for security guards, by receiving video feed from large crowds, and ultimately improve crowd safety?

{{< pagebreak >}}

# State of the art

## Convolutional Neural Network (CNN) for Computer Vision
Convolutional Neural Networks (CNN) are a type of Artificial Neural Networks (ANN) that are used to "solve difficult image-driven pattern recognition tasks." [@DBLP:journals/corr/OSheaN15] ANN's function on an input in the form of a multi-dimensional vectors, which will be distributed to a number of hidden layers. The hidden layers will be weighted by evaluating how a stochastic change within itself affects the final output (backpropagation). Deep learning is an ANN with multiple hidden layers. ANN's can be either supervised: Being trained on a set of labeled training data, or unsupervised: Having no labeled training data, but instead minimizing the result of the cost function. According to @DBLP:journals/corr/OSheaN15 and @NIPS2010_fe73f687, supervised learning is usually necessary for image-focused pattern recognition tasks.

CNN's are similar to ANN's in almost every way, with the only difference being that CNN's allow to encode image specific features such as edges, textures, color patterns and shape features. This reduces the complexity of a traditional ANN trained on image data, where a vector representation grows in size exponentially depending on the image size. Reducing the feature vector also reduces the risk of overfitting the model. This is done through the process of convolution. In the convolution layer, local regions of the image vector are scanned for image relevant features. Usually the features are run through a pooling layer, reducing its computational complexity by sampling and down-scaling the features. This feature map is then fed to a traditional ANN, with a given amount of hidden layers. 

## Solving Scale-Variation with SASNet
Scale variation is one of the main challenges with crowd counting, since perspective in the image will lead to different sizes of heads. One solution to this is proposed by @sasnet. SASNet relies on the fact that heads in a local path share roughly the same scale. It is currently not a common approach to solve scale variation on a continuous scale. SASNet solves it discretly but in 5 discrete steps using a weighted average. This bridges the gap between discrete feature extraction and continuous feature extraction. This architecture can be seen in @fig-sasnet.

![Architecture of the SASNet model [Diagram from @sasnet]](../images/sasnet-architechture.png){#fig-sasnet}

As it can be seen from the diagram, SASNet uses the first 13 convolutional layers from VGG-16 [@simonyan2014very] in the encoder. SASNet then produces 5 feature levels with 5 levels of downsampling ($V_n$). It produces 5 levels of predictions ($P_n$). This is what is referred to as the "U-shaped backbone". These are then fed to a confidence branch and a density branch that produces a confidence map and density map for each of the 5 levels. The complete density map is a product of the sum of the individual density and confidence branches using a softmax function for the confidence. 

# Analysis

## System specification

Using cameras mounted around a hotspot of a crowd or on a stationary drone, we aim to develop a platform where an AI model receives live footage from these cameras, uses the model to segment the crowd, and learns what a dangerous situation might look like. This could be one or more of the following [@inproceedings]: 

* Several people moving into the crowd from a specific direction create a dangerous pressure point
* More than a specified amount of people in a marked area resulting in unsafe conditions (ie. >6 people per square meter)
* Omnidirectional or directional movement in the crowd resulting in a dangerous situation
* An area in the crowd suddenly being void of people, perhaps hinting at a mosh pit or an emergency

These are some of the situations this project aims to systematically detect and alert professionals by providing meaningful feedback.

The system would consist of the following: 

1. A backend that receives the data from the cameras developed in either Java or Python, with an implementation of a machine learning model to handle the video feed according to the bullet list above and exposing this data through an API
1. A simple frontend or GUI (Graphical User Interface) to display this information in a meaningful way.  This could be through a heat map, a numerical estimate of a risk factor, or other visual output.

![Component diagram of the system](../images/component-diagram-1.png){height=50% #fig-component-diagram}

An initial component diagram that maps this described system can be seen in @fig-component-diagram.

## Functional Requirements
| **ID**   | **MoSCoW**   | **Requirement**  |
| --- | -------------- | ----------------------------------------------------- |
| 1   | M        | The system **must** be able to count the amount of people in the crowd with a precision of <100 MAE|
|     |          |                                                             |
| 2   | M        | The system **must** be able to segment the crowd into virtual sections for further processing |
|     |          |                                                             |
| 3   | M        | The system **must** be able to create a heatmap of the crowd density |
|     |          |                                                             |
| 4   | S        | The system **should** be able to calculate the "real" density pr. area unit |
|     |          | 
| 5   | S        | The system **should** be able to detect the movement of crowd sections |
|     |          |                                                             |
| 6   | S        | The system **should** be able to correct for camera distortion, warp and perspective |
|     |          |                                                             |
| 7   | S        | The system **should** be able to detect choke points in the crowd movements |
|     |          |                                                             |
| 8   | S        | The system **should** be able to generate a summarizing report of the concert with statistics of (crowd density, crowd count, risk factors, choke points, and other) relevant safety indicators after the event |
|     |          |                                                             |
| 9   | C        | The system **could** be able to estimate a numerical risk factor based on available factors |
|     |          |                                                             |
| 10   | C        | The system **could** be able to generate a live, or delayed, video overlaid user interface |
|     |          |                                                             |
| 11  | W        | The system **will not** be able to identify dangerous situations that might call for cautionary actions such as crowd surge, mosh pits, falls, blocked exits, etc. |
|     |          |                                                             |
| 12  | W        | The system **will not** be able to use data gathered elsewhere at a concert such as alcohol sales, average crowd age, sound, artists, etc. to give a more precise crowd profile and thus risk factor |

{{< pagebreak >}}

## Non-functional requirements

| **ID**   | **MoSCoW**   | **Requirement**  |
| --- | -------------- | ----------------------------------------------------- |
| 1   | M        | The system **must** protect the privacy of the personal data |
|     |          |                                                             |
| 2   | S        | The system **should** have a user-friendly interface that is easy to manage for both technical and non-technical users |
|     |          |                                                             |
| 3   | S        | The system **should** have adequate documentation / technical specification for technical users |
|     |          |                                                             |
| 4   | S        | The system **should** have adequate user manuals for non-technical users |
|     |          |                                                             |
| 5   | C        | The system **could** have high reliability that is not based on the visual circumstances and environment (e.g. sunlight, stage light, audience flashlights, and other visual effects) or report confidence based on environment |
|     |          |                                                             |
| 6   | C        | The system **could** be scalable to simultaneous interoperability between multiple cameras |
|     |          |                                                             |
| 7   | C        | The system **could** integrate with existing CCTV software systems at venues |

{{< pagebreak >}}

## Detailed Requirements

## Risks
More often that not a project will encounter problems that can hinder the progress or outcome. By being well prepared we are able to mitigate some of these risks. The following table describes some of the risks we might encounter in this project.

| **ID** 	| **Name** 	| **Affects** 	| **Description** 	| **Mitigation** 	|
|---	|---	|---	|---	|---	|
| R01 	| Code is lost 	| Project, product 	| If the code for some reason is lost or google colab i suddenly insaccessible 	| Having backups of our code on github 	|
|       	|         	|             	|                   |                   	|
| R02 	| Technology constraints 	| Product 	| If the technology previously used in the project suddenly becomes inadequate for the remaining requirements defined in the project 	| Find alternative technologies or solutions for our requirements before a barrier might be reached. Discuss with supervisor if alternatives exist. 	  |
|       	|         	|             	|                   |                   	|
| R03 	| Personal Conflicts 	| Product, project 	| A project related or personal conflict between the 2 members of the project having an effect on either further development or the project as a whole. 	| Keeping a friendly and open mindset in the work will take us far. Depending on the nature of a potential conflict we would either discuss and solve it outside of work or discuss with our supervisor. |
|       	|         	|             	|                   |                   	|
| R04 	| Unusable data 	| Project, Product 	| If the data provided by the drone or from EventSafety turns out to be unusable due to one or more factors: Resolution, perspective, Distortion etc. 	| Making sure that provided data is on par regarding the requirements of our project. Some data might be fixed by using mathematics but in general a proper standard for data is preferred 	|
|       	|         	|             	|                   |                   	|
| R05 	| Company collaboration ends 	| Project 	| If EventSafety or the group decides to end the collaboration around the project early.  	| Making sure that we listen to and adapt to feedback from EventSafety to keep our good relationship with them	|

## Risk assessment

To fully assess the risks described above it is absolutely vital that the probabality and severity of each individual risk is assessed. This results in the following formula: 

$$
Effect = Probabilty * Severity
$$

The probablity and severity is given each given a score between 1 and 5. 1 being a low probabilty/severity and 5 being a very high probabilty/severity. The effect is then calculated based on these numbers. 

| **ID** 	| **Probablity** 	| **Severity** 	| **Effect** 	| **Notes** 	|
|---	|---	|---	|---	|---	|
| R01 	| 1 	| 3 	| 3 	| The severity is based on the amount of code lost 	|
|   	|   	|   	|   	|                           	|
| R02 	| 2 	| 2 	| 4 	| GPU limitations are the primary factor here. However, we dont see the need for more than what google colab can provide.  	|
|   	|   	|   	|   	|                           	|
| R03 	| 1 	| 1 	| 1 	| Given the nature of previous work in semester projects and friendship, this is highly unlikely 	|
|   	|   	|   	|   	|                           	|
| R04 	| 4 	| 3 	| 12 	| The severity really depends on how bad the data is and how well we are able to adapt to it 	|
|   	|   	|   	|   	|                           	|
| R05 	| 1 	| 5 	| 5 	| Severity would depend on when in the process EventSafety would cancel our collaboration.  	|

# Method and theory
TODO: should we keep this?
## CUDA

{{< pagebreak >}}

# Design

## About Crowd Counting
Crowd counting is a field of computer vision research that has developed quickly in recent years. It is a technique that aims to count the instances of any object in an image, no matter the context, density or type of object. [@sasnet] This differentiates it from object detection which is usually trained on one specific type of object by extracting certain visual features. The benefits from using crowd counting rather than object detection is: 

* Crowd counting is less intensive on computing resources.

* Crowd counting performs better in crowded scenes with varying scales of objects and overlapping objects.

* Crowd counting is more robust in environments with changes in light, without the need of specific training data.

* Crowd counting is not as exposed to the risk of over fitting the model. [@li2021approaches]

* Crowd counting does not single out individuals, but looks at the crowd holistically [@chan2008]

For these reasons crowd counting is a method that can be used for many domains including traffic and parking analysis, pedestrian crowds, corn and crop counting, and much more. This project will use crowd counting methods to analyse, count and estimate density of concert crowds.

[@li2021approaches] says the current leading approach to crowd counting is the use of a CNN. This paper, contrary to our definiton of crowd counting, includes object detection as a method of crowd counting. It also includes regression based methods and, of course, CNN's. The main challenge with CNN's for crowd counting is to differentiate between the large scale variations in the countable human heads. [@li2021approaches] discuss several categories of approaches for CNN based models to solve the scale variation problems, one of which is multi-scale fusion. This approach is the category that this paper proposes for further research, as it shows promising results in the balance between performance and accuracy.

For the reasons outlined here from @DBLP:journals/corr/OSheaN15 and @li2021approaches, this project will focus on the use of CNN based models for crowd counting.

One of the useful datasets for crowd counting, "Shanghaitech" comes from the paper @Zhang_2016_CVPR. In this paper there are defined two datasets, "part a" and "part b". For the purposes of the CSMS test data, "Shanghaitech Part A" is a more representative dataset to use, since it contains more densely populated crowds from a birds eye view. The dataset is labeled with one dot representing roughly the middle of a person's head. This is the approach outlined in one of the founding papers of crowd counting @NIPS2010_fe73f687. In @NIPS2010_fe73f687, it is explained how their approach to crowd counting, which has become the primary method in the field, is to create a density function $F$ as a function of the pixels in an image $I$. This density function is analoguous to the physical idea of density, however not a direct representation of the same concept, as physical density is based on physical area. Integrating $F$ over the entire image $I$ will return the number of people in $I$. [@NIPS2010_fe73f687] Each object in the image I should be represented by a normalized 2D Gaussian kernel with the mean at the person's head. One disadvantage with this approach, is that the kernel for objects close to the edge of the image will not be counted as a whole object when summed. It is not assumed that this will pose a significant source of error for the CSMS.

## Security & privacy

Being citizens of and using the data of people from an EU country, we have to abide by the rules of GDPR. This means that security and privacy around the data that we have acquired are crucial. Practically, this means that we have a responsibility to make sure that we only upload the images and videos we need to run the model on when they need to be there and remove them when not needed. In a perfect world, we would run our program locally to limit the uploading and removal of sensitive data.

### Facial recognition vs. Crowd Counting

When reading through this project, one might assume that privacy around facial recognition would be an issue. However, as this is related to heatmaps, crowd counting & crowd density of people, not facial recognition the identity of people in our footage is not a major concern and thus was not featured in our risk management section. The CSMS does not track individuals through time, which could be a primary privacy concern.\


### UCloud data policy
As mentioned, data handling and data privacy are very important for this project. Due to this, we had to research the policies of UCloud regarding data handling before implementing our software on the platform. According to @UCloud_Security, UCloud is ISO 27001 certified. This means that the platform lives up to an international standard for information security management systems @iso27001. Furthermore, according to @UCloud_Security, UCloud is hosted locally on SDU and not by a 3rd party operator. Using this information, we were able to conclude that UCloud as a platform is safe to use for our project and regulated to relevant standards.


## Regarding SAM model for Crowd Counting:  
One of the original ideas for solving this problem was to use Meta's SAM(Segment Anything Model) model. During the preliminary research sessions, it was discovered that SAM was not adequate in counting people but better suited for object detection. The following citation from @ma2023sam highlights the problems with using SAM:

"Although the Segment Anything model (SAM) has shown impressive performance in many scenarios, it currently lags behind state-of-the-art few-shot counting methods, especially for small and congested objects. We believe that this is due to two main reasons. Firstly, SAM tends to segment congested objects of the same category with a single mask. Secondly, SAM is trained with masks that lack semantic class annotations, which could hinder its ability to differentiate between different objects. Nevertheless, further exploration of adapting SAM to the object counting task is still worth studying." [@ma2023sam]

## The choice of SASNet
The choice of using SASNet was based on three different factors:

1. SASNets fits our need to solve the problem of scale variation in the video material. It was not possible to obtain video material from an approximate orthographic perspective. However, future endeavors for this project might include cameras like those from one of our collaborators [PhaseOne](https://www.phaseone.com/) who produces approximate orthographic perspectives using high pixel density cameras from high altitudes. SASNet, however, fulfills the requirement to produce relatively high accuracy on an image with perspective for now.

2. SASNet has some of the lowest absolute errors on the ShanghaiTech testing data, according to @gjy3035_awesome_crowd_counting which compares at least 45 different crowd counting models and methods. 

3. SASNet is open source and licensed under the permissive Apache 2.0 license, which allows us to use it legally and for free in our project. SASNet also publishes the pretrained model weights (trained on ShanghaiTech A and B), which means we can save many compute- and time resources on training the model ourselves. 

Finally, SASNet publishes the model weights from the training on either ShanghaiTech part A or part B. For this project, it is deemed more useful to use the images from part A. While the labeled dataset is smaller in part A than part B, it more closely represents the data (i.e. amount of people, density and perspectives) that will be used in this project.


## Activity diagram

@fig-activity-diagram illustrates the flow of the program through an activity diagram. The diagram was created to obtain a deeper understanding of the general flow and processes the program goes through during its runtime. The diagram is split into 2 "swimlanes", one lane for our CSMS system and one lane for the implementation of the SASNet model. In broad terms, this can also be specified as a CPU and a GPU lane where our CSMS system handles CPU processing and SASNet focuses on GPU processing.

As depicted in the diagram, section of the activities are divided into partitions to visualise a rough partioning into classes and for better readability throughout the diagram. The input is specified as the path to the pretrained model and the path to the video for it to run on.

Below is a short runthrough of the different partitions. A more indepth description can be found in the implemetation section

1. System Initialization and Input Validation

The CSMS is initialized and the inputs for the model and video path are validated

2. SASNet Model Setup and Construction

The SASNet model is loaded together with VGG16 model which it is based on. This sets the foundation for subsequent image processing.

3. Pre-Processing of Input Data

Preprocessing involves creating a dataset from the video in pyTorch and in turn creating a dataloader from this dataset.

4. Image processing Loop

The images in the dataloader are iterated through and forwarded to the SASNet model until the dataloader is empty. Each image processed through the different layers to prepare them for post-processing

5. Post-Processing & Result Generation

Following the image processing, the CSMS takes over again. Here density information is computed, perspective warping is applied to in turn generate the correct heatmap for further analysis.

6. Output Generation & Reporting

All of the crowd counting data is saved to a CSV filed and the images are stitched together to form a timelapse and a report, summarizing the observations.

![Activity diagram depicting flow in the solution](../images/activity-diagram-1.png){#fig-activity-diagram}

## Design considerations regarding user experience
TODO: Add reasoning behind downscaling of the map. And reasoning behind heatmaps. Counting in selected areas. 
For many purposes a video with a heatmap could be sufficient. However, in order to make it possible to satisfy requirements such as functional requirement 2 and 8 as well as non-functional requirement 2, it was decided to create a simple user interface

{{< pagebreak >}}

# Implementation {#sec-backend}

## Backend

### Data Collection and Preparation
To implement this project it was necessary to gather useful evaluation data. For the purposes of this project, that is video security footage and drone footage. Our collaborative partner EventSafety was very helpful in letting us gather this data at their 2 festivals, "Smukfest" and "Grøn". This was done 1-2 months before the project started since this is when the festivals were held. This means that we gathered the data before completing the research on crowd counting and computer vision. It did pose a challenge to collect the data before knowing exactly what type of data was needed. For these reasons, we collected many types of data, including:

1. Drone footage from "Grøn" at an altitude of 25-50 meters from the entrance

1. Drone footage from "Grøn" at an altitude of 25-50 meters of the concerts and end of concerts

1. GoPro footage from "Grøn" at an altitude of 2-5 meters from the entrance

1. Security camera footage from "Smukfest at an altitude of approximately 10-15 meters from the center of the stage. 

1. Security camera footage from "Smukfest" at various heights from bars and paths

What turned out to be most useful was option number 2 and option number 4. Having a perspective from a higher altitude means that more people are clearly visible and also minimizes distortion when warping for perspective correction. Having an even higher resolution from a higher altitude could also increase the accuracy of the predictions. The resolution of the drone footage was 1080p. Having a higher resolution could limit the video compression artifacts. The camera options were discussed with our secondary partner PhaseOne. They have experience with ultra-high-resolution top-down drone footage. This could be an option for future endeavors, which will be discussed in @sec-perspective section of this report.

While we, with the help of a certified drone pilot, collected the footage from "Grøn" ourselves, the security footage from "Smukfest" was collected by EventSafety. For this, we created a technical specification with our requirements for the footage at the time. This can be read in @fig-camera-specs in @sec-appendices. 

### Environment

Setting up an environment for machine learning in Python can pose a bit of a challenge. Is a requirement to have access to a NVIDIA GPU to take advantage of the CUDA toolkit in PyTorch. This opens up GPU acceleration which enables a performance boost rather than running the CNN models on the CPU. Furthermore, the SASNet open source project was built for Python 3.6.8 and PyTorch >= 1.5.0 @sasnet.

### Google Colab
TODO: Could be deleted

Because of these strict software and hardware requirements, we decided to use the virtual environment in [Google Colab](https://research.google.com/colaboratory/faq.html). Google Colab provides a preconfigured Python environment with (free but not unlimited) access to a NVIDIA GPU. Doing this project in the cloud removes the software and hardware limitations on both the users and developers of the project. However, Google Colab has some feature limitations such as it being disallowed to use it for the purpose of hosting web services. Google Drive can then be volume mounted inside the virtual environment making it possible to use and make persisted files. 

The Python version in Google Colab is downgraded to match the version needed for SASNet and both PyTorch and the required PIP packages are installed. 

### UCloud
University of Southern Denmark provides a cloud computing service for faculty members through [eScience](https://escience.sdu.dk/index.php/national-hpc-systems/). Our supervisor applied through this site for a cloud computing resource with 50 GPU hours. The project was granted 100 GB storage and the "u1-gpu @ DeiC Interactive HPC (SDU)" server. In UCloud we used the application "Coder CUDA" to run the project. We created a bash script (crowd-safety/backend/init.py) with the following responsibilities to initiate the environment:
* Initiating and cloning Git submodules
* Installing necessary Ubuntu packages
* Installing necessary Python installation
* Installing Python package manager (PIP)
* Installing all Python package dependencies

### SASNet Model Adaption
The SASNet model is loaded using pre-trained weights and biases for the underlying vgg16_bn model trained on ImageNet. This allows SASNet to more efficiently identify image features with low memory usage. Furthermore, the SASNet weights and bias fine-tunings are loaded into the model which has been trained on the Shanghai Tech Part A dataset. A block size of 32 is being used here.

TODO: Add explanation of how block_size is being used in SASNet

## Downscaling 

## Frontend
In order to create a flexible user interface, the output data was stripped of all post-processing artifacts (heatmap, scale, count and other text) as this would now be calculated in the frontend. The frontend is created using [Qt for Python](https://wiki.qt.io/Qt_for_Python) to allow for cross platform compatability, to have access to video playback components and have access to the Python OpenCV module to allow for preprocessing of the video. There are 3 main widgets in the frontend. The main VideoPlayer widget which has the buttons to upload the video, clear mask, change heatmap colorscheme. The InfoWidget which displays textual data such as the count, count in selection, density, density in selection and time passed. The FloatingOverlay which is an overlay that follows the size and position of the VideoPlayer where the user can draw a polygon to define an area on the video that should be counted. A simplified version of this structure can be seen in @fig-frontend-class-diagram and the full diagram with all methods and attributes can be seen in the appendix. 
![Simplified class diagram depicting the structure of the frontend](../images/class-diagram-frontend.png){#fig-frontend-class-diagram}
The VideoPlayer calls a preprocess function which adds the selected heatmap and upscales the downscaled video before sending it to the QVideoWidget. A density scale is also created after the video has been uploaded. All of this is done using OpenCV.

## Integeration
Although a proper API between the frontend and backend was described in the component diagram, @fig-component-diagram, it has not been a priority of the project and thus not developed. For further development, it is still necessary to enable direct upload from the frontend. Currently, all video content must be uploaded to UCloud in order to processed which then produces an output .avi file that can be read and processed properly by the frontend.

## Optimization and Performance
For two main reasons, it is necessary to limit the compute resource usage:

1. This project is using free and limited GPU and compute resources. Computer vision and CNN's can be very GPU intensive, so it is in the project's to limit the GPU memory consumption as this was found to be the largest resource bottle neck.

2. For the CSMS to be scalable to (near) real-time evaluation performance, it makes sense to limit the use of computing power as much as possible.

According to @pytorch, [disabling gradient calculation](https://pytorch.org/docs/stable/generated/torch.no_grad.html) is useful for lowering memory usage when using a model for inference. This fits the use case and greatly reduced the memory usage. This was done using the following way:

```python
img = img.cuda()
with torch.no_grad():
  model.eval()
  pred_map = model(img)
pred_map = pred_map.data.cpu().numpy()
```

Here the image tensor is first sent to the NVIDIA GPU using img.cuda(). Gradient calculations are then disabled for the inference, and the model is set to eval mode. These two reduce the GPU memory usage. Finally, after the inference is run and the predicition_map is calculated, the tensor data is sent to the cpu and the tensor is converted to a numpy matrix (an image). This way limits the time that the tensor spends in the GPU greatly, as well as reducing the memory usage.

{{< pagebreak >}}

# Validation and Verification
Since this project is a proof of concept, unit tests and integration tests are not as favored as user tests and similar. Testing of this system relies on making sure that it is useful and understandable to those who need it. To accomplish these tests, we decided to get the opinions of some EventSafety security personnel and professionals about our project and its usefulness. 

## Runtime and GPU usage
While GPU usage is something already discussed in implementation, runtime and the optimization of this is still crucial to test. While we originally wanted this project to be able to run in real-time, we quickly realized that this was close to impossible with our current setup and implementation. When running the model on a video, we originally ran it on a 3-minute clip with an image every 10 seconds being processed. This resulted in a 6-second video and a runtime of the program of around 15 minutes. Not great, not terrible. In the future, if we want to run this live, additional resources are required. More about this in @sec-perspective

## EventSafety presentation

Based on the analysis, design and development of the crowd management system we were invited to present our project to more people from EventSafety. Based on this invitation it was decided to research how a proper focus group is structured and managed properly.

### Focus group {#sec-focusgroup}

"As a summative evaluation, focus groups can be used to assess the acceptance of a new campaign or gauge customer satisfaction levels" @designers_research_manual
Based on research from @designers_research_manual we arrived at the following conclusion for our research group:

- A group of 6-9 people is the preferred group size
- A group of similarly experienced people is needed to encourage a proper discussion
- If possible, have more groups with different people

For the presentation, a group of 10 employees from Eventsafety attended the presentation with questions and feedback for us along the way. The employees all had major experience with crowd management and event planning. Most of them had heard of our project beforehand but had no further technical insight. 

### Questionaire

Following the focus group, a questionnaire is conducted to gain further insight.
Normally, a questionnaire is used to gather information from a large group. For our presentation, we still felt a questionnaire was relevant to gather solid data for a project report and validation of the project as a whole.

The questionnaire was conducted using Mentimeter and consisted of scales, open-ended questions and rankings. This section will highlight some of the answers the attendees provided. All the questions and their results (in danish) are found in @sec-appendices.\

**Question 2**\
![Questionnaire question 2](../images/Evaluation_question2.png){#fig-evaluation_question_2}


Question 2 had the attendees evaluate 3 statements on a scale of 1 to 5 and "strongly disagree" to "strongly agree". 1 being disagree and 5 being agree.\

The 3 statements were as follows:\
1. I find the usability of the platform adequately simple\
2. The data the program visualizes is easy to read and understand\
3. The data the program provides is credible compared to my own observations\
The first statement was evaluated to a score of 3.9, showing that while the platform in general seems intuitive and fulfills non-functional requirement #2, there is room for visual improvements.\

Statement 2 further validates the implementation of non-functional requirement 2# with a score of 4.3\

Statement 3 shows that the professional observations from our attendees align with the output of our program, showing that functional requirement #1 is fulfilled.\
An attendee also mentioned that the heatmap provided by the program gives more value than a precise count of individuals in a crowd. This comment was unfortunately not documented. \

**Question 3**\
![Questionnaire question 3](../images/Evaluation_question3.png){#fig-evaluation_question_2}

Question 3 had the attendees evaluate 2 statements from 0 to 100% based on their preferred uncertainty of the data provided.\ 

1. What percentage of uncertainty would you prefer?\
2. How high of an uncertainty would you be willing to accept?\

Question 1 shows a score of 2.2, meaning an uncertainty of around 22% would be ideal for the crowd-counting values provided by the program. This aligns with functional requirement #1, showing that our initial idea of a precision of <100 MAE is way above what we initially thought would be useful.\
Question 2 shows a score of 3.7, meaning an uncertainty of around 37% would still be useful. This further ties into the comment made to question 2, that the precision of the counts is not as important as the visualized density maps - though these do rely on each other.\

## Further evaluation of functional requirements {#sec-eval_functional_requirements}
Based on the results and evaluation of our presentation for EventSafety personnel, we can say the following about our functional requirements

1. The system **must** be able to count the number of people in the crowd with a precision of <100 MAE.\
The system can use the model to count crowds of people where a human would be able to. When lighting conditions or materials from a concert(bright lights, confetti, fireworks etc.) are present, crowd counting precision is dramatically reduced. 

2. The system **must** be able to segment the crowd into virtual sections for further processing.\
When the processed video is uploaded to our frontend, the user can select a polygon for further processing. The program provides a count and density value for the specified area.

3. The system **must** be able to create a heatmap of the crowd density.\
The system creates a heatmap of the analyzed video to visualize crowd density. The user can select the color scheme of the heatmap themselves before uploading a video. 

4. The system **should** be able to calculate the "real" density per area unit.\
The system can calculate and display the "real" density based on calculations that sum pixel values in a given area as each pixel value corresponds to a certain amount of people.

5. The system **should** be able to detect the movement of crowd sections.\
The system is not directly able to detect the movement of a crowd. However, the system does create a timelapse of the area in a video which can be used to manually follow certain crowd movements.

6. The system **should** be able to correct for camera distortion, warp, and perspective.\
The system can correct for perspective and warp by providing ground measurements and image coordinates to the cv2.warpPerspective() method of the openCV library. 

7. The system **should** be able to detect choke points in the crowd movements.
The system is not able to detect chokepoints beyond manual observations of the users.

8. The system **should** be able to generate a summarizing report of the concert with statistics of (crowd density, crowd count, risk factors, choke points, and other) relevant safety indicators after the event.\
The System is not able to generate a report of every statistic provided here. The report is based on the current frame of the timelapse and shows crowd count and crowd density.

9. The system **could** be able to estimate a numerical risk factor based on available factors.\
The system is not able to estimate a numerical risk factor. 

10. The system **could** be able to generate a live, or delayed, video overlaid user interface.\
The system can generate a delayed video user interface based on the input. 


## Further evaluation of non-functional requirements{#sec-eval_non_functional_requirements}
Likewise, we can say the following about our non-functional requirements

1. The system **must** protect the privacy of personal data.\
The system only handles sensitive when analyzing the surveillance footage. The only way the data is ever close to online is when it is processed on UCloud. However, as the only online part of the process is the usage of UCloud GPUs, the data is never compromised.

2. The system **should** have a user-friendly interface that is easy to manage for both technical and non-technical users.\
The system has a user-friendly interface that is easy to manage for both technical and non-technical users. This is based on the answers provided in @sec-focusgroup

3. The system **should**  have adequate documentation /technical specifications for technical users.\
TODO

4. The system **should**  have adequate user manuals for non-technical users.\
TODO
5. The system **could** have high reliability that is not based on the visual circumstances and environment (e.g. sunlight, stage light, audience flashlights, and other visual effects) or report confidence based on the environment.\
See @sec-eval_functional_requirements

6. The system **could** be scalable to simultaneous interoperability between multiple cameras.\
The system can provide output from multiple sources of input at the same time. 

7. The system **could** integrate with existing CCTV software systems at venues.\
The system is not able to integrate with existing CCTV beyond the footage provided to it.

Based on 




{{< pagebreak >}}

# Discussion {#sec-discussion}

Privacy of crowd footage combined with other data

TODO: Add section on possibilities with PhaseOne
{{< pagebreak >}}

# Conclusion {#sec-conclusion}

{{< pagebreak >}}

# Perspective {#sec-perspective}

{{< pagebreak >}}

# References

# Appendices {.appendix #sec-appendices}

## Mentimeter presentation result
![Mentimeter presentation result](../appendices/MentimeterPresentation.pdf)(#fig-presentation-results)
## Technical specifications for camera
![Technical specification document for cameras](../appendices/teknisk-specifikation.png){#fig-camera-specs width=100% height=100%}
## Full class diagram for frontend
![Class diagram for frontend](../images/class-diagram-frontend-full.png.png){#fig-frontend-class-diagram-full}


## Timeline

| **Date** 	| **Activity** 	|
|---	|---	|
| July-August | Gathering data at attended festivals in collaboration with Event Safety |
||
| August 	| Drafting the project description 	|
||
| 31-08-2023 	| Delivering the project description 	|
||
| 09-09-2023 	| Final discussion with Event Safety regarding the proposed solutions, requirements and the project going forward 	|
| September 	| In-depth analysis of use cases (with Event Safety) In-depth analysis of required technologies and methods used in academic literature.  Design of the system 	|
||
| 25-09-2023 	| Progress update with Event Safety regarding the design of the system 	|
||
| October 	| Implementation of proof of concept system (primarily backend) 	|
||
| 26-10-2023 	| Progress update with Event Safety and small-scale user testing 	|
||
| 31-10-2023 	| Attending Crowd Safety Course hosted by Event Safety in Copenhagen 	|
||
| November 	| Implementation of proof of concept system 	|
||
| December 	| Final implementation of the system Documentation of the system User testing and evaluation 	|    
